{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Problem (unicode1): Understanding Unicode**\n",
    "\n",
    "(a) what Unicode character does chr(0) return?\n",
    "\n",
    "\\x00 (NUL)\n",
    "\n",
    "(b) How does this character’s string representation (__repr__()) differ from its printed representation?\n",
    "\n",
    "print(chr(0)) is unvisible.\n",
    "\n",
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:\n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    ">>> print(chr(0))\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "```\n",
    "\n",
    "  'this is a test\\x00string'\n",
    "\n",
    "    this is a teststring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem (unicode2): Unicode Encodings**\n",
    "\n",
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "\n",
    "For most real-world text (especially English), UTF-8 is much more space-efficient and UTF-8 is byte-oriented. UTF-8 is the de facto standard for web, files, and APIs. Most text data you encounter is already in UTF-8.\n",
    "\n",
    "UTF-16/32 can introduce null bytes in the middle of text, which can break C-style string handling and some legacy systems, while UTF-8 never encodes ASCII characters (U+0000 to U+007F) with null bytes (\\x00).\n",
    "\n",
    "Byte-level tokenizers (like GPT-2’s BPE) work naturally with UTF-8, since every possible byte value (0–255) is valid\n",
    "\n",
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "\n",
    "Decoding byte-by-byte breaks multi-byte characters.\n",
    "\n",
    "(c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "\n",
    "[0xC0, 0x80]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem (train_bpe_tinystories): BPE Training on TinyStories**\n",
    "\n",
    "(a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "Resource requirements: ≤30 minutes (no GPUs), ≤ 30GB RAM\n",
    "\n",
    "Hint: You should be able to get under 2 minutes for BPE training using multiprocessing during\n",
    "\n",
    "pretokenization and the following two facts:\n",
    "\n",
    "    (a) The <|endoftext|> token delimits documents in the data files.\n",
    "\n",
    "    (b) The <|endoftext|> token is handled as a special case before the BPE merges are applied.\n",
    "    \n",
    "Deliverable: A one-to-two sentence response.\n",
    "\n",
    "(b) Profile your code. What part of the tokenizer training process takes the most time?\n",
    "Deliverable: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken before pretokenization: 1.02 seconds\n",
      "Starting pre-tokenization with 8 processes on 16 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 16/16 [00:40<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenization and initial counting time: 40.97 seconds\n",
      "Total words processed: 539,309,867\n",
      "Unique word patterns: 59,934\n",
      "Initial vocab size: 257\n",
      "Target vocab size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing BPE merges: 100%|██████████| 9743/9743 [00:06<00:00, 1470.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge time: 6.63 seconds\n",
      "Training took 49.74058437347412 seconds\n"
     ]
    }
   ],
   "source": [
    "# train_bpe_tinystories\n",
    "\n",
    "import bbpe_train\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "vocab, merges = bbpe_train.train_bbpe(\n",
    "    input_path=\"../data/TinyStoriesV2-GPT4-train.txt\",\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    num_chunks=16,\n",
    "    num_processes=8,\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Training took {end_time - start_time} seconds\")\n",
    "\n",
    "with open(\"tinystories_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open(\"tinystories_merges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(merges, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial `merge_pair` implementation was very slow. The latest version uses a highly optimized algorithm with several key improvements:\n",
    "\n",
    "1. Word frequency based tokenization.\n",
    "2. 16 chunks, 8 processes.\n",
    "\n",
    "Profiling:\n",
    "\n",
    "Use scalene to analyze the bottle neck.\n",
    "\n",
    "```\n",
    "cd cs336_basics\n",
    "scalene --html --outfile analysis_report.html scalene_unified_analysis.py\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem (train_bpe_expts_owt): BPE Training on OpenWebText\n",
    "\n",
    "(a) Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary\n",
    "size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What\n",
    "is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "**Resource requirements**: ≤12 hours (no GPUs), ≤ 100GB RAM\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response.\n",
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken before pretokenization: 5.38 seconds\n",
      "Starting pre-tokenization with 8 processes on 32 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 32/32 [03:27<00:00,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenization and initial counting time: 209.36 seconds\n",
      "Total words processed: 2,474,152,489\n",
      "Unique word patterns: 6,601,893\n",
      "Initial vocab size: 257\n",
      "Target vocab size: 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing BPE merges: 100%|██████████| 31743/31743 [36:40<00:00, 14.43it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge time: 2200.31 seconds\n",
      "Training took 2601.484979391098 seconds\n"
     ]
    }
   ],
   "source": [
    "# train_bpe_expts_owt\n",
    "import bbpe_train\n",
    "import pickle\n",
    "import time\n",
    "start_owt_time = time.time()\n",
    "owt_vocab, owt_merges = bbpe_train.train_bbpe(\n",
    "    input_path=\"../data/owt_train.txt\",\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    num_chunks=32,\n",
    "    num_processes=8,\n",
    ")\n",
    "end_owt_time = time.time()\n",
    "print(f\"Training took {end_owt_time - start_owt_time} seconds\")\n",
    "with open(\"owt_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(owt_vocab, f)\n",
    "\n",
    "with open(\"owt_merges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(owt_merges, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
